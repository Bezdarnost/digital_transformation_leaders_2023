{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca84326-9879-4459-a5ba-7428011f0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from lion_pytorch import Lion\n",
    "from torchmetrics.classification import BinaryMatthewsCorrCoef\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b1e24-64fd-4969-ae05-64ad213b52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "mini_batch = 64\n",
    "batch_size = 64\n",
    "accumulation_steps = batch_size // mini_batch\n",
    "\n",
    "num_epochs = 100\n",
    "df = pd.read_csv('train_v3.csv')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca0087-b60c-4361-9697-f2b2422032ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        data1 = self.data.iloc[index, 3:327].values\n",
    "        data1 = torch.from_numpy(data1).float()\n",
    "        \n",
    "        data2 = self.data.iloc[index, 327:].values\n",
    "        data2 = torch.from_numpy(data2).float()\n",
    "        \n",
    "        target = self.data.iloc[index, 0]\n",
    "        return data1, data2, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6cd5f-1913-435a-9ab4-cb23cf32e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data = LCTDataset(data=df)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=mini_batch, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(f'Number of batches: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e766ad-e94a-44ba-a442-74bf72265e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(324, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.body(input1)\n",
    "        output2 = self.body(input2)\n",
    "        output = torch.abs(torch.subtract(output1, output2))\n",
    "        output = self.classifier(output)\n",
    "        return outputclass SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(324, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.body(input1)\n",
    "        output2 = self.body(input2)\n",
    "        output = torch.abs(torch.subtract(output1, output2))\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "    \n",
    "model = SiameseNet();\n",
    "model.load_state_dict(torch.load('model_v5_39.pth'))\n",
    "    \n",
    "model = SiameseNet();\n",
    "model.load_state_dict(torch.load('model_v5_39.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33b743-2642-4f27-bc62-9ea2113ee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);\n",
    "\n",
    "print(summary(model, [(1, 324), (1, 324)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6b422-f3f2-4095-8204-5978741c5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = df.copy()\n",
    "weights['count_of_targets'] = weights['target'].map(weights['target'].value_counts())\n",
    "weights.sort_values('target', inplace=True)\n",
    "weights.drop_duplicates('target', inplace=True)\n",
    "n_samples = list(weights['count_of_targets'])\n",
    "weights = [(max(n_samples)/n) for n in n_samples]\n",
    "loss_weights = torch.FloatTensor(weights).to(device)\n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558ead1-cd4a-4300-aa3c-1ccf3bd1388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W_BCELoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, w_n = 3.2452, w_p = 1.0000):\n",
    "        super(W_BCELoss, self).__init__()\n",
    "        \n",
    "        self.w_p = w_p\n",
    "        self.w_n = w_n\n",
    "        \n",
    "    def forward(self, logits, labels, epsilon = 1e-7):\n",
    "        \n",
    "        loss_pos = -1 * torch.mean(self.w_p * labels * torch.log(logits + epsilon))\n",
    "        loss_neg = -1 * torch.mean(self.w_n * (1-labels) * torch.log((1-logits) + epsilon))\n",
    "        \n",
    "        loss = loss_pos + loss_neg\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f37c26-9a90-4f45-b4f1-873e0f4a54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device, epoch, accumulation_steps):\n",
    "    flag = True\n",
    "    model.train() \n",
    "    final_loss = 0  \n",
    "    train_acc = 0\n",
    "    total=0\n",
    "    metric = BinaryMatthewsCorrCoef(threshold=0.75).to(device)\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    for batch_idx, (features1, features2, labels) in loop:\n",
    "        inputs1, inputs2, targets = features1.to(device, non_blocking=True), features2.to(device, non_blocking=True), labels.to(device, non_blocking=True).float()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        outputs = outputs.reshape(len(outputs))\n",
    "        metric.update(outputs, targets)\n",
    "        loss = loss_fn(outputs, targets) \n",
    "        loss.backward() \n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  \n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad() \n",
    "#        total += len(targets)\n",
    "        final_loss += loss.item() \n",
    "#        predicted = torch.where(outputs >= 0.5, 1, 0)\n",
    "#        train_acc += ((predicted == targets).sum().item())\n",
    "        if (epoch + 1) % 10 == 0 and flag:\n",
    "            name = 'model_v9_' + str(epoch + 1) + '.pth'\n",
    "            torch.save(model.state_dict(), name)\n",
    "            flag = False\n",
    "        loop.set_description(f'Epoch: [{epoch+1}/{num_epochs}]') \n",
    "        loop.set_postfix(loss=final_loss/(batch_idx+1), mat=metric.compute().item())#, acc=train_acc/total*100)\n",
    "    final_loss /= len(dataloader) \n",
    "#    train_acc = (train_acc/total)*100\n",
    "    \n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9eafa6-abfe-47af-9f65-d961343b5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_loss_lst, num_epochs, accumulation_steps):\n",
    "    #using adam optimizer for optimization\n",
    "    optimizer = optim.AdamW(filter(lambda x: x.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max = num_epochs, eta_min = 3e-4)\n",
    "    loss_fn = W_BCELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_fn(model, optimizer, scheduler, loss_fn, train_loader, device, epoch, accumulation_steps) #training loss and accuracy\n",
    "        train_loss_lst.append(train_loss)\n",
    "    \n",
    "    print(f'Train finished!')\n",
    "    print(f'Your last train accuracy is {train_acc_lst[-1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a345ae9-be51-4482-b042-3591bf0d8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "train_loss_lst = []\n",
    "run_training(train_loss_lst, num_epochs, accumulation_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
